{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bandit Experiment\n",
    "Test contrastive vs reconstruction embeddings on real Amazon product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.embeddings import get_extractor\n",
    "from src.models import LinearContextualBandit, SimpleNeuralBandit\n",
    "from src.datasets import AmazonDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download real Amazon Electronics data (streams, stops at n_items_per_category)\n",
    "# Default: 3 categories x 3333 items = ~10K total items\n",
    "dataset = AmazonDataset(\n",
    "    categories=['Electronics'],  # Start with one category\n",
    "    n_items_per_category=3000,\n",
    "    cache_dir='../data/amazon'\n",
    ")\n",
    "print(f\"Loaded {len(dataset)} items\")\n",
    "\n",
    "# Get text for embeddings\n",
    "texts = dataset.get_item_texts()\n",
    "print(f\"Sample: {texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive (expected to work well)\n",
    "simcse = get_extractor('simcse')\n",
    "simcse_embs = simcse.encode(texts)\n",
    "print(f\"SimCSE embeddings: {simcse_embs.shape}\")\n",
    "\n",
    "# Reconstruction-based (expected to struggle)\n",
    "bert = get_extractor('bert')\n",
    "bert_embs = bert.encode(texts)\n",
    "print(f\"BERT embeddings: {bert_embs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Effective Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.eigenvalues import compute_eigenvalues, compute_effective_dimension\n",
    "\n",
    "bert_eigs = compute_eigenvalues(bert_embs)\n",
    "simcse_eigs = compute_eigenvalues(simcse_embs)\n",
    "\n",
    "bert_deff = compute_effective_dimension(bert_eigs)\n",
    "simcse_deff = compute_effective_dimension(simcse_eigs)\n",
    "\n",
    "print(f\"BERT d_eff: {bert_deff:.1f}\")\n",
    "print(f\"SimCSE d_eff: {simcse_deff:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Bandit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit_experiment(embeddings, dataset, n_rounds=5000, n_candidates=500, seed=42):\n",
    "    \"\"\"\n",
    "    Run Thompson Sampling bandit with given embeddings.\n",
    "    \n",
    "    Key design choices for valid regret measurement:\n",
    "    - User preference is a random embedding direction (tests embedding geometry)\n",
    "    - Reward depends on embedding similarity (directly tests hypothesis)\n",
    "    - Regret computed using EXPECTED rewards (not stochastic samples)\n",
    "    - Oracle best = max expected reward over candidates\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    bandit = LinearContextualBandit(embedding_dim=embeddings.shape[1], algorithm='ts')\n",
    "    \n",
    "    rewards = []\n",
    "    regrets = []\n",
    "    cumulative_regret = 0\n",
    "    \n",
    "    for t in tqdm(range(n_rounds)):\n",
    "        # Sample random user preference (in embedding space)\n",
    "        user_pref = np.random.randn(embeddings.shape[1])\n",
    "        user_pref = user_pref / np.linalg.norm(user_pref)\n",
    "        \n",
    "        # Get candidate items\n",
    "        candidates = np.random.choice(len(dataset), n_candidates, replace=False)\n",
    "        candidate_embs = embeddings[candidates]\n",
    "        \n",
    "        # Select action using bandit\n",
    "        action_idx = bandit.select_action(candidate_embs)\n",
    "        selected_item = candidates[action_idx]\n",
    "        \n",
    "        # Compute EXPECTED rewards for all candidates (for regret)\n",
    "        expected_rewards = np.array([\n",
    "            dataset.compute_reward_prob(c, user_pref, embeddings)\n",
    "            for c in candidates\n",
    "        ])\n",
    "        \n",
    "        # Oracle best action\n",
    "        best_expected = np.max(expected_rewards)\n",
    "        selected_expected = expected_rewards[action_idx]\n",
    "        \n",
    "        # Instantaneous regret (using expected values)\n",
    "        instant_regret = best_expected - selected_expected\n",
    "        cumulative_regret += instant_regret\n",
    "        \n",
    "        # Sample actual reward (for bandit update)\n",
    "        reward = dataset.sample_reward(selected_item, user_pref, embeddings)\n",
    "        \n",
    "        # Update bandit\n",
    "        bandit.update(candidate_embs[action_idx], reward)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        regrets.append(instant_regret)\n",
    "    \n",
    "    return np.array(rewards), np.array(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "print(\"Running BERT bandit...\")\n",
    "bert_rewards, bert_regrets = run_bandit_experiment(bert_embs, dataset)\n",
    "\n",
    "print(\"Running SimCSE bandit...\")\n",
    "simcse_rewards, simcse_regrets = run_bandit_experiment(simcse_embs, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative regret\n",
    "ax1 = axes[0]\n",
    "ax1.plot(np.cumsum(bert_regrets), label=f'BERT (d_eff={bert_deff:.0f})', color='red')\n",
    "ax1.plot(np.cumsum(simcse_regrets), label=f'SimCSE (d_eff={simcse_deff:.0f})', color='blue')\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Cumulative Regret')\n",
    "ax1.set_title('Cumulative Regret: Contrastive vs Reconstruction')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling average reward\n",
    "ax2 = axes[1]\n",
    "window = 200\n",
    "bert_rolling = np.convolve(bert_rewards, np.ones(window)/window, mode='valid')\n",
    "simcse_rolling = np.convolve(simcse_rewards, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(bert_rolling, label='BERT', color='red', alpha=0.7)\n",
    "ax2.plot(simcse_rolling, label='SimCSE', color='blue', alpha=0.7)\n",
    "ax2.set_xlabel('Round')\n",
    "ax2.set_ylabel('Average Reward (200-round window)')\n",
    "ax2.set_title('Learning Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/amazon_bandit_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal cumulative regret:\")\n",
    "print(f\"  BERT: {np.sum(bert_regrets):.1f}\")\n",
    "print(f\"  SimCSE: {np.sum(simcse_regrets):.1f}\")\n",
    "print(f\"  Reduction: {100*(np.sum(bert_regrets) - np.sum(simcse_regrets))/np.sum(bert_regrets):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
