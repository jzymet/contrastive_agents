{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolBench A2C Experiment\n",
    "Test contrastive vs reconstruction embeddings on LLM tool selection using A2C with RewardTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.embeddings import get_extractor\n",
    "from src.models import A2CTransformerAgent\n",
    "from src.datasets import ToolBenchDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ToolBench Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ToolBench dataset (16K+ APIs)\n",
    "dataset = ToolBenchDataset(cache_dir='../data/toolbench')\n",
    "print(f\"Loaded {len(dataset)} tools\")\n",
    "\n",
    "# Get tool descriptions for embeddings\n",
    "tool_texts = dataset.get_tool_texts()\n",
    "print(f\"Sample: {tool_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive (expected to work well)\n",
    "simcse = get_extractor('simcse')\n",
    "simcse_embs = simcse.encode(tool_texts)\n",
    "print(f\"SimCSE embeddings: {simcse_embs.shape}\")\n",
    "\n",
    "# Reconstruction-based (expected to struggle)\n",
    "bert = get_extractor('bert')\n",
    "bert_embs = bert.encode(tool_texts)\n",
    "print(f\"BERT embeddings: {bert_embs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Effective Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.eigenvalues import compute_eigenvalue_spectrum, compute_effective_dimension\n",
    "\n",
    "bert_eigs, _ = compute_eigenvalue_spectrum(bert_embs)\n",
    "simcse_eigs, _ = compute_eigenvalue_spectrum(simcse_embs)\n",
    "\n",
    "bert_deff = compute_effective_dimension(bert_eigs)\n",
    "simcse_deff = compute_effective_dimension(simcse_eigs)\n",
    "\n",
    "print(f\"BERT d_eff: {bert_deff:.1f}\")\n",
    "print(f\"SimCSE d_eff: {simcse_deff:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Synthetic I3 Tasks\n",
    "I3 tasks require multi-tool, cross-category chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_tasks(tool_embs: np.ndarray, n_tasks: int = 100, tools_per_task: int = 3, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create synthetic I3-style tasks where we know the ground truth tool sequence.\n",
    "    \n",
    "    Each task:\n",
    "    - Has a random query embedding direction\n",
    "    - Requires a sequence of 3 tools\n",
    "    - Ground truth tools are those closest to the query in different directions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tasks = []\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        # Random query direction\n",
    "        query_emb = np.random.randn(tool_embs.shape[1]).astype(np.float32)\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "        \n",
    "        # Find tools closest to rotated versions of query (simulating multi-step reasoning)\n",
    "        gt_tools = []\n",
    "        for step in range(tools_per_task):\n",
    "            # Rotate query for each step\n",
    "            rotation = np.random.randn(tool_embs.shape[1]).astype(np.float32)\n",
    "            rotated = query_emb + 0.3 * step * rotation\n",
    "            rotated = rotated / np.linalg.norm(rotated)\n",
    "            \n",
    "            # Find best tool not already selected\n",
    "            similarities = tool_embs @ rotated\n",
    "            for idx in gt_tools:\n",
    "                similarities[idx] = -np.inf\n",
    "            best_idx = int(np.argmax(similarities))\n",
    "            gt_tools.append(best_idx)\n",
    "        \n",
    "        tasks.append({\n",
    "            'query_id': i,\n",
    "            'query_emb': query_emb,\n",
    "            'ground_truth_tools': gt_tools,\n",
    "            'required_categories': [],  # Not used for synthetic\n",
    "        })\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Create tasks using SimCSE embeddings (tasks are fixed, embeddings vary)\n",
    "tasks = create_synthetic_tasks(simcse_embs, n_tasks=200)\n",
    "print(f\"Created {len(tasks)} synthetic I3 tasks\")\n",
    "print(f\"Example task ground truth tools: {tasks[0]['ground_truth_tools']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run A2C Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a2c_experiment(\n",
    "    tool_embs: np.ndarray,\n",
    "    tasks: list,\n",
    "    n_episodes: int = 100,\n",
    "    n_candidates: int = 50,\n",
    "    max_steps: int = 5,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Run A2C agent on tool selection tasks.\n",
    "    \n",
    "    Metrics:\n",
    "    - Task success rate (found all required tools)\n",
    "    - Average reward per episode\n",
    "    - Steps to completion\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    agent = A2CTransformerAgent(\n",
    "        d_model=tool_embs.shape[1],\n",
    "        nhead=8,\n",
    "        num_layers=2,\n",
    "        hidden_dim=512,\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_successes = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes), desc=\"Episodes\"):\n",
    "        task = tasks[ep % len(tasks)]\n",
    "        query_emb = task['query_emb']\n",
    "        gt_tools = set(task['ground_truth_tools'])\n",
    "        \n",
    "        tools_used = []\n",
    "        sequence_embs = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Sample candidate tools (include some ground truth for learning)\n",
    "            n_gt = min(3, len(gt_tools - set(tools_used)))\n",
    "            gt_remaining = list(gt_tools - set(tools_used))\n",
    "            candidates = gt_remaining[:n_gt] if n_gt > 0 else []\n",
    "            \n",
    "            # Add random candidates\n",
    "            available = [i for i in range(len(tool_embs)) if i not in tools_used]\n",
    "            random_candidates = np.random.choice(available, min(n_candidates - n_gt, len(available)), replace=False)\n",
    "            candidates.extend(random_candidates.tolist())\n",
    "            candidates = candidates[:n_candidates]\n",
    "            \n",
    "            candidate_embs = tool_embs[candidates]\n",
    "            \n",
    "            # Select action\n",
    "            action_idx, log_prob, value = agent.select_action(\n",
    "                query_emb, sequence_embs, candidate_embs\n",
    "            )\n",
    "            selected_tool = candidates[action_idx]\n",
    "            \n",
    "            # Compute reward\n",
    "            if selected_tool in gt_tools:\n",
    "                if selected_tool == task['ground_truth_tools'][len(tools_used)] if len(tools_used) < len(task['ground_truth_tools']) else False:\n",
    "                    reward = 2.0  # Correct tool in correct position\n",
    "                else:\n",
    "                    reward = 1.0  # Correct tool, wrong position\n",
    "            else:\n",
    "                reward = -0.2  # Wrong tool\n",
    "            \n",
    "            tools_used.append(selected_tool)\n",
    "            sequence_embs.append(tool_embs[selected_tool])\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Check completion\n",
    "            done = gt_tools.issubset(set(tools_used)) or step == max_steps - 1\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(\n",
    "                query_emb, sequence_embs[:-1], candidate_embs,\n",
    "                action_idx, log_prob, value, reward, done\n",
    "            )\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update agent after each episode\n",
    "        if len(agent.trajectory_buffer) >= 10:\n",
    "            agent.update()\n",
    "        \n",
    "        success = gt_tools.issubset(set(tools_used))\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_successes.append(success)\n",
    "        episode_steps.append(len(tools_used))\n",
    "    \n",
    "    return {\n",
    "        'rewards': np.array(episode_rewards),\n",
    "        'successes': np.array(episode_successes),\n",
    "        'steps': np.array(episode_steps),\n",
    "        'success_rate': np.mean(episode_successes),\n",
    "        'avg_reward': np.mean(episode_rewards)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "print(\"Running BERT A2C...\")\n",
    "bert_results = run_a2c_experiment(bert_embs, tasks, n_episodes=100)\n",
    "\n",
    "print(\"\\nRunning SimCSE A2C...\")\n",
    "simcse_results = run_a2c_experiment(simcse_embs, tasks, n_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Success rate over time (rolling window)\n",
    "ax1 = axes[0]\n",
    "window = 20\n",
    "bert_rolling_success = np.convolve(bert_results['successes'].astype(float), np.ones(window)/window, mode='valid')\n",
    "simcse_rolling_success = np.convolve(simcse_results['successes'].astype(float), np.ones(window)/window, mode='valid')\n",
    "ax1.plot(bert_rolling_success, label=f'BERT (d_eff={bert_deff:.0f})', color='red')\n",
    "ax1.plot(simcse_rolling_success, label=f'SimCSE (d_eff={simcse_deff:.0f})', color='blue')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Success Rate (20-ep window)')\n",
    "ax1.set_title('Task Success Rate')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative reward\n",
    "ax2 = axes[1]\n",
    "ax2.plot(np.cumsum(bert_results['rewards']), label='BERT', color='red')\n",
    "ax2.plot(np.cumsum(simcse_results['rewards']), label='SimCSE', color='blue')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Cumulative Reward')\n",
    "ax2.set_title('Cumulative Reward')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Steps to completion\n",
    "ax3 = axes[2]\n",
    "bert_rolling_steps = np.convolve(bert_results['steps'], np.ones(window)/window, mode='valid')\n",
    "simcse_rolling_steps = np.convolve(simcse_results['steps'], np.ones(window)/window, mode='valid')\n",
    "ax3.plot(bert_rolling_steps, label='BERT', color='red')\n",
    "ax3.plot(simcse_rolling_steps, label='SimCSE', color='blue')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Avg Steps (20-ep window)')\n",
    "ax3.set_title('Steps to Completion')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/toolbench_a2c_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "print(f\"BERT:   Success Rate = {bert_results['success_rate']*100:.1f}%, Avg Reward = {bert_results['avg_reward']:.2f}\")\n",
    "print(f\"SimCSE: Success Rate = {simcse_results['success_rate']*100:.1f}%, Avg Reward = {simcse_results['avg_reward']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Embedding Geometry → Tool Selection Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how embedding geometry affects tool selection\n",
    "from src.analysis.coverage import compute_coverage_metric\n",
    "\n",
    "# Coverage: how well does a random k-sample cover the action space?\n",
    "bert_coverage_dict = compute_coverage_metric(bert_embs, k_values=[50], n_trials=50)\n",
    "simcse_coverage_dict = compute_coverage_metric(simcse_embs, k_values=[50], n_trials=50)\n",
    "\n",
    "bert_coverage = bert_coverage_dict[50]\n",
    "simcse_coverage = simcse_coverage_dict[50]\n",
    "\n",
    "print(f\"Coverage ρ(50, {len(bert_embs)}) - lower is better:\")\n",
    "print(f\"  BERT:   {bert_coverage:.4f}\")\n",
    "print(f\"  SimCSE: {simcse_coverage:.4f}\")\n",
    "\n",
    "print(f\"\\nTheory prediction: High d_eff → Low ρ → Better exploration → Higher success rate\")\n",
    "print(f\"  SimCSE d_eff={simcse_deff:.0f} > BERT d_eff={bert_deff:.0f}\")\n",
    "print(f\"  SimCSE coverage={simcse_coverage:.4f} < BERT coverage={bert_coverage:.4f}\")\n",
    "print(f\"  SimCSE success={simcse_results['success_rate']*100:.1f}% > BERT success={bert_results['success_rate']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
