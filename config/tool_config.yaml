dataset:
  name: "toolbench"
  n_tools: 500
  task_types:
    - I1
    - I2
    - I3

experiment:
  n_episodes: 100
  max_steps: 10
  k: 500

embeddings:
  simcse_model: "princeton-nlp/sup-simcse-bert-base-uncased"
  bert_model: "bert-base-uncased"
  batch_size: 32
  cache_dir: "cache"

bandit:
  type: "thompson_sampling"

a2c:
  embedding_dim: 768
  n_heads: 8
  n_layers: 2
  dropout: 0.1
  learning_rate: 0.0003
  gamma: 0.99

metrics:
  - success_rate
  - cumulative_regret
  - coverage_rho
  - steps_per_episode
